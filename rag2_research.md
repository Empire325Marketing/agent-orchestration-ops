# Research Report: Designing a Production-Ready RAG 2.0 System for the Primarch Project

**Date:** 2025-09-30

### Executive Summary
This report presents a comprehensive research analysis to guide the design and implementation of a production-ready Retrieval-Augmented Generation (RAG) 2.0 system for the Primarch project. The primary objective is to architect a system capable of meeting stringent acceptance gates: a retrieval recall at 10 (R@10) of at least 0.90, end-to-end answer faithfulness of 0.95 or greater, and a 95th percentile (p95) retrieval latency of 150 milliseconds or less from a PostgreSQL database utilizing the pgvector extension. The analysis concludes that these targets are achievable through a multi-faceted strategy centered on advanced retrieval techniques, high-performance reranking, and meticulous database optimization. The recommended architecture integrates a hybrid search mechanism, combining keyword-based BM25 and dense vector search with Reciprocal Rank Fusion (RRF) for superior initial retrieval. For handling complex, multi-step queries, an agentic reasoning layer is proposed to perform dynamic query decomposition and multi-hop retrieval, potentially leveraging knowledge graphs. To ensure high relevance, a second-stage reranking process using the BAAI/bge-reranker-v2-m3 model on GPU infrastructure is advised. Finally, to meet the demanding latency requirements, this report details a suite of optimization strategies for Postgres/pgvector, including the use of HNSW indexing, quantization, and careful hardware provisioning. This integrated approach provides a robust framework for building a scalable, efficient, and highly accurate RAG 2.0 system tailored for enterprise-grade deployment.

### Introduction
The evolution from basic Retrieval-Augmented Generation (RAG) systems to the next-generation RAG 2.0 paradigm marks a significant shift towards more intelligent, accurate, and performant information retrieval systems. The objective of this report is to provide a detailed technical blueprint for the design and implementation of such a system for the Primarch project. This endeavor is guided by a set of rigorous acceptance criteria that define a production-ready state: exceptional retrieval accuracy, near-perfect answer faithfulness, and consistently low query latency. Achieving these goals requires moving beyond simple vector similarity search and embracing a more sophisticated, multi-stage architecture that addresses the inherent limitations of individual components. Key challenges in this domain include overcoming the semantic gaps of pure vector search, handling complex queries that require reasoning across multiple documents, and ensuring that the computational overhead of advanced techniques does not compromise real-time performance. This report deconstructs these challenges and proposes evidence-based solutions across three core architectural pillars: advanced retrieval strategies, high-performance relevance reranking, and optimized data persistence and querying within a PostgreSQL and pgvector environment. The analysis synthesizes findings from recent benchmarks, practical implementation guides, and academic research to provide actionable recommendations for technical architects and engineers tasked with building this mission-critical system.

### Advanced Retrieval Strategies for RAG 2.0
To achieve the high recall and faithfulness required by the Primarch project, the retrieval stage must be architected to handle a wide spectrum of user queries, from simple fact-finding to complex, multi-faceted questions. This necessitates a move away from single-method retrieval towards a more robust, layered approach that combines the strengths of different search paradigms and incorporates intelligent query processing. The foundation of this advanced strategy is a hybrid search system, which is further enhanced by an agentic layer capable of multi-hop reasoning and dynamic query decomposition.

A hybrid search architecture is essential because it mitigates the weaknesses inherent in using either keyword-based search or semantic vector search in isolation. Keyword-based systems, such as those using the BM25 algorithm, excel at matching specific terms, names, and identifiers, which is critical for queries where precision is paramount. However, they fail to capture semantic nuance, synonyms, or user intent. Conversely, dense vector search, powered by embedding models, understands contextual relationships and can retrieve conceptually similar results even if the exact keywords are absent. Yet, it can struggle with queries containing specific jargon, acronyms, or entities that were underrepresented in the embedding model's training data. By combining these two methods, the system can capture both lexical and semantic relevance. For instance, a query for "GAN research" would benefit from BM25 ensuring that documents explicitly mentioning the acronym "GAN" are retrieved, while the dense vector component would find related papers discussing "generative adversarial networks" even if the acronym is not used. The fusion of these disparate result sets is most effectively handled by a technique known as **Reciprocal Rank Fusion (RRF)**. Unlike linear score combination, which is difficult to calibrate due to the different scales of BM25 and vector similarity scores, RRF operates on the rank of each result. It calculates a final score by summing the reciprocal of each document's rank in the respective result lists, often with a smoothing constant `k` (e.g., `k=60`) to balance the influence of top-ranked items. This method is robust, requires no score normalization, and has become a de facto standard for practical hybrid search implementations within platforms like OpenSearch, Weaviate, and PostgreSQL.

While hybrid search significantly improves the quality of single-step retrieval, it is insufficient for complex queries that require synthesizing information from multiple sources. To address this, a RAG 2.0 system must incorporate **agentic multi-hop reasoning**. This involves an AI agent, typically powered by a large language model (LLM), that orchestrates a dynamic, multi-step retrieval process. The agent's primary function is **query decomposition**, where it breaks down a complex user query into a series of simpler, answerable sub-queries. For example, a query like "Which authors who have written about reward hacking have also received funding from projects related to video diffusion?" would be decomposed into distinct retrieval tasks. The agent would first search for documents about "reward hacking" to identify authors, then perform subsequent searches for each author to find their funding sources, and finally filter those sources for connections to "video diffusion." Frameworks like LangGraph provide the tools to construct such agentic workflows, defining a stateful graph where nodes represent actions like generating a query, retrieving documents, grading the relevance of retrieved content, and rewriting the query if the initial results are unsatisfactory. This iterative process of retrieving, reasoning, and refining allows the system to build a comprehensive context before generating a final answer, directly contributing to higher answer faithfulness. This approach is further enhanced by integrating retrieval from structured knowledge graphs. By representing entities and their relationships as nodes and edges, the agent can traverse the graph to uncover connections that would be difficult to find in unstructured text, a technique demonstrated by advanced systems like INRAExplorer and GraphRAG. This structured traversal implicitly performs multi-hop retrieval, enabling the system to answer questions that depend on a chain of interconnected facts.

### High-Performance Reranking for Enhanced Relevance
The initial retrieval stage, even when enhanced with hybrid search and agentic reasoning, is designed for speed and breadth, aiming to retrieve a sufficiently large set of candidate documents to ensure high recall. However, this initial set may contain documents of varying relevance. To meet the stringent R@10 ≥ 0.90 acceptance gate, a second, more precise filtering stage is required. This is the role of a reranker, a model that re-evaluates and re-orders the initial candidate set to push the most relevant documents to the top. For the Primarch project, the deployment of a powerful cross-encoder model as a reranker is a critical component for achieving state-of-the-art relevance.

Cross-encoder models, unlike the bi-encoders used for initial retrieval, process a query and a document simultaneously as a single input. This joint processing allows the model to perform a much deeper, token-level analysis of the interaction and relevance between the query and the document, resulting in significantly higher accuracy. The **BAAI General Embedding (BGE) v2 reranker family** offers several compelling options for production deployment. Among these, the **`bge-reranker-v2-m3`** model stands out as a prime candidate, offering an excellent balance of high accuracy, multilingual capabilities, and optimized performance. Benchmarks have consistently shown that rerankers like `bge-reranker-v2-m3` can substantially improve the accuracy of RAG pipelines, with some studies reporting improvements of up to 14% over baseline embedding models alone. This level of improvement is essential for ensuring that the top 10 documents presented to the generator model are of the highest possible quality, which directly impacts both the final answer's accuracy and its faithfulness to the source material.

The primary challenge with cross-encoders is their computational intensity, which can introduce significant latency if not managed correctly. Performance benchmarks starkly illustrate this trade-off. For instance, when ranking 100 candidate documents for a single query, the `bge-reranker-v2-m3` model takes approximately 257 seconds on a standard CPU. This level of latency is entirely unacceptable for a production system. However, the same task completed on a modern GPU sees a dramatic reduction in processing time. On an NVIDIA T4 GPU, the latency drops to under 3.4 seconds, and on a more powerful NVIDIA A10 GPU, it falls to just 1.4 seconds. This represents a performance increase of over 180 times compared to the CPU. This data unequivocally demonstrates that **GPU acceleration is a non-negotiable requirement** for deploying a reranker in the Primarch RAG system. To further optimize performance, it is recommended to utilize half-precision floating-point arithmetic (**FP16**), which can provide an additional speedup with a negligible impact on accuracy. The deployment architecture should therefore consist of a dedicated inference service for the reranker model, hosted on GPU-equipped hardware and integrated into the RAG pipeline after the initial retrieval from PostgreSQL. This two-stage process—fast, broad retrieval followed by a slower, highly accurate reranking—provides a balanced architecture that maximizes both recall and precision without sacrificing the end-user experience.

### Optimizing the Vector Database: Postgres and pgvector
The performance of the entire RAG system is fundamentally constrained by the speed of its retrieval backbone. The project's requirement to use PostgreSQL with the `pgvector` extension as the vector database, coupled with the stringent acceptance gate of a p95 retrieval latency of 150 milliseconds or less, necessitates a deep focus on database optimization. Fortunately, recent advancements in `pgvector` and a clear understanding of performance tuning principles provide a confident path to meeting this target. The key levers for optimization are advanced indexing, strategic use of quantization, appropriate hardware provisioning, and careful consideration of embedding dimensionality.

The single most important factor for achieving low-latency vector search at scale is the use of an appropriate index. While `pgvector` supports multiple index types, the **Hierarchical Navigable Small World (HNSW)** index is the recommended choice for its superior performance and favorable speed-recall trade-off compared to alternatives like IVFFlat. HNSW constructs a multi-layered graph of vectors, allowing for highly efficient nearest-neighbor searches. Performance can be finely tuned using its build-time and query-time parameters. The `ef_construction` parameter controls the quality of the index graph, while the `ef_search` parameter determines the breadth of the search at query time. Increasing `ef_search` generally improves recall at the cost of higher latency, so it must be carefully calibrated based on empirical testing with the project's specific dataset and query patterns. Extensive benchmarks demonstrate the effectiveness of this approach. On datasets containing up to 50 million vectors, properly tuned `pgvector` with HNSW indexing has been shown to achieve p95 latencies well under 100 milliseconds with 99% recall, comfortably within the 150ms project limit. These benchmarks also show `pgvector` outperforming several specialized vector databases in both throughput and latency on a single node, validating its suitability for this production system.

Beyond indexing, **quantization** offers another powerful optimization technique. Quantization reduces the memory footprint of vectors by storing them with lower precision, such as using 2-byte floats (fp16) or even binary representations. This not only reduces storage costs and memory usage but can also dramatically accelerate index builds and query speeds. For example, benchmarks have shown that binary quantization can reduce index build times by a factor of 150 and improve query throughput by over 30 times. While there can be a slight trade-off in accuracy, this can often be mitigated by retrieving a slightly larger number of candidates to be passed to the high-precision reranker. Hardware provisioning is also critical; the HNSW index graph should ideally fit within the server's RAM to avoid costly disk I/O. Therefore, selecting a database instance with sufficient memory is paramount. Finally, the choice of **embedding dimensionality** has a direct impact on performance. While higher-dimensional vectors (e.g., 1024d) may capture more semantic detail and offer marginal recall improvements on certain complex benchmarks, they also consume more resources and increase query latency. Lower-dimensional vectors (e.g., 768d) often provide a much better balance of accuracy, performance, and cost, especially for on-premises deployments. Reducing dimensionality from 1536d to 768d can effectively halve memory usage and double query throughput with minimal impact on accuracy. Therefore, selecting a high-performing 768-dimensional embedding model is the recommended starting point for the Primarch project.

### Data Ingestion and Parsing Strategy
The quality of a RAG system's output is fundamentally dependent on the quality of the data it retrieves, a principle often summarized as "garbage in, garbage out." A robust and intelligent data ingestion and parsing strategy is therefore a foundational requirement for the Primarch project. The goal of this initial stage is to transform raw source documents—ranging from structured text and complex PDFs to entire code repositories—into clean, contextually meaningful chunks that can be effectively indexed and retrieved. A one-size-fits-all approach to parsing is insufficient; the strategy must employ specialized tools tailored to the structure of each data type.

For unstructured and semi-structured documents, particularly PDFs, which often contain complex layouts with tables, figures, and multi-column text, a simple text extraction method is inadequate. Such methods often jumble the reading order and lose critical structural information. To overcome this, it is recommended to use an advanced document parsing framework like **Unstructured**. Integrated within ecosystems such as LangChain via the `UnstructuredPDFLoader`, this tool goes beyond basic text extraction by employing layout analysis models to intelligently segment documents into their constituent elements, such as titles, paragraphs, lists, and tables. It can also perform Optical Character Recognition (OCR) to extract text from images within the documents. By parsing a PDF into these logical "elements" rather than arbitrary fixed-size chunks, the system preserves the semantic integrity of the content, leading to higher-quality embeddings and more relevant retrieval results. This approach ensures that the context provided to the LLM during generation is coherent and complete.

When dealing with source code from repositories, a different set of challenges arises. Code is not natural language; its structure, defined by functions, classes, methods, and import statements, is paramount to its meaning. Naively splitting code files using character or token counters will inevitably break these logical structures, resulting in fragmented and often meaningless chunks. The recommended solution is to leverage a dedicated code parsing tool like **tree-sitter**. Tree-sitter is a powerful parser generator that can build a complete and concrete syntax tree for a given source file across a multitude of programming languages. By traversing this syntax tree, the ingestion pipeline can implement an intelligent chunking strategy. For example, it can create distinct chunks for each function, class, or even logical block of code, preserving the full context of that unit. This "code-aware" chunking ensures that when a snippet is retrieved, it represents a complete, syntactically correct, and logically coherent piece of code. This structured approach is vastly superior for RAG applications involving code, as it provides the generator model with context that is directly relevant and immediately usable, whether for code generation, explanation, or debugging tasks.

### Conclusion and Recommendations
The analysis conducted in this report provides a clear and actionable framework for architecting a RAG 2.0 system that meets the demanding performance, accuracy, and faithfulness requirements of the Primarch project. By systematically addressing the key challenges at each stage of the RAG pipeline—from data ingestion to final answer generation—the proposed architecture establishes a robust foundation for an enterprise-grade, production-ready system. The synthesis of advanced retrieval methods, high-precision reranking, and aggressive database optimization creates a synergistic system where each component compensates for the limitations of the others, leading to a final product that is greater than the sum of its parts. The following recommendations outline the core pillars of this proposed architecture.

**Recommendation 1: Implement a Multi-Faceted Advanced Retrieval System.** The initial retrieval stage should be built upon a hybrid search foundation within PostgreSQL, combining the lexical precision of BM25 full-text search with the semantic power of `pgvector`. The results from these two methods should be merged using Reciprocal Rank Fusion (RRF) to produce a unified and highly relevant candidate set. To handle complex user queries, this system must be augmented with an agentic reasoning layer. This layer will be responsible for query decomposition, breaking down intricate questions into a series of manageable sub-queries and orchestrating a multi-hop retrieval process to gather all necessary evidence before synthesis.

**Recommendation 2: Deploy a GPU-Accelerated Reranker for High-Fidelity Relevance.** To refine the candidate set from the retrieval stage and meet the R@10 ≥ 0.90 target, a second-stage reranker is essential. The `BAAI/bge-reranker-v2-m3` model is recommended for its exceptional balance of accuracy and performance. This model must be deployed on dedicated GPU infrastructure (e.g., NVIDIA A10 or equivalent) and leverage FP16 precision to ensure that its computational overhead does not create a bottleneck. This two-stage retrieval-then-rerank process is critical for maximizing the relevance of the context provided to the generator model.

**Recommendation 3: Aggressively Optimize the PostgreSQL and pgvector Database.** Achieving the p95 retrieval latency target of ≤ 150ms requires meticulous database tuning. The primary vector search index should be HNSW, with its `ef_search` parameter carefully calibrated to balance recall and speed. Further performance gains should be sought through techniques like scalar or binary quantization to reduce the memory footprint and accelerate queries. The system should be provisioned with sufficient RAM to ensure the HNSW index graph resides in memory, and an initial embedding dimensionality of 768 is recommended for an optimal trade-off between performance and accuracy.

**Recommendation 4: Adopt a Specialized Data Ingestion and Parsing Strategy.** The quality of the indexed data is paramount. For document processing, especially complex PDFs, the `Unstructured` library should be employed to perform intelligent, layout-aware parsing that preserves the semantic structure of the content. For ingesting code repositories, `tree-sitter` should be used to parse source code into syntax trees, enabling a "code-aware" chunking strategy that creates logically coherent and contextually complete chunks based on functions, classes, and other programming constructs.

By implementing these integrated recommendations, the Primarch project can confidently proceed with the development of a sophisticated RAG 2.0 system that not only meets but exceeds its stringent acceptance criteria, delivering a scalable, reliable, and highly intelligent information retrieval solution.

### References
[Speed Showdown for your RAG improvement: Reranker performance on CPU/GPU/TPU - Medium](https://medium.com/@xiweizhou/speed-showdown-reranker-1f7987400077)
[BAAI/bge-reranker-large - Hugging Face](https://huggingface.co/BAAI/bge-reranker-large)
[BAAI/bge-reranker-v2-m3 - Hugging Face](https://huggingface.co/BAAI/bge-reranker-v2-m3)
[BAAI/bge-m3 - Hugging Face](https://huggingface.co/BAAI/bge-m3)
[FlagOpen/FlagEmbedding - GitHub](https://github.com/FlagOpen/FlagEmbedding)
[BAAI/bge-reranker-base - Hugging Face](https://huggingface.co/BAAI/bge-reranker-base)
[Benchmarking Cohere Reranker with LanceDB - LanceDB](https://blog.lancedb.com/benchmarking-cohere-reranker-with-lancedb/)
[BAAI/bge-reranker-large Model Card - Dataloop](https://dataloop.ai/library/model/baai_bge-reranker-large/)
[BGE Reranker v2 m3 - Pinecone](https://docs.pinecone.io/models/bge-reranker-v2-m3)
[Top Rerankers for RAG - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/06/top-rerankers-for-rag/)
[MTEB: Massive Text Embedding Benchmark - Hugging Face](https://huggingface.co/blog/mteb)
[A Survey on Retrieval-Augmented Text Generation for Large Language Models - arXiv](https://arxiv.org/html/2406.01607v1)
[Embedding models and dimensions: optimizing the performance/resource usage ratio - Microsoft Azure SQL](https://devblogs.microsoft.com/azure-sql/embedding-models-and-dimensions-optimizing-the-performance-resource-usage-ratio/)
[NVIDIA Text Embedding Model Tops MTEB Leaderboard - NVIDIA Developer Blog](https://developer.nvidia.com/blog/nvidia-text-embedding-model-tops-mteb-leaderboard/)
[Massive Text Embedding Benchmark (MTEB) helps to find optimal embedding for your RAG/LLM use case - Medium](https://prasun-mishra.medium.com/massive-text-embedding-benchmark-mteb-helps-to-find-optimal-embedding-for-your-rag-llm-use-case-1af15eab40e1)
[Massive Text Embedding Benchmark (MTEB) - Zilliz](https://zilliz.com/glossary/massive-text-embedding-benchmark-(mteb))
[embeddings-benchmark/mteb - GitHub](https://github.com/embeddings-benchmark/mteb)
[Mastering Text Embedding and Reranker with Qwen3 - Medium](https://k-farruh.medium.com/mastering-text-embedding-and-reranker-with-qwen3-3284e7ae14c7)
[Embedding Models Rundown - Pinecone](https://www.pinecone.io/learn/series/rag/embedding-models-rundown/)
[Massive Text Embedding Benchmark - PaperToHTML](https://papertohtml.org/paper?id=ea16991340ec05e0cbededeff5cf8fe4620d34bb)
[Agentic RAG with Knowledge Graphs for Complex Multi-Hop Reasoning in Real-World Applications - arXiv](https://arxiv.org/abs/2507.16507)
[Using Knowledge Graphs and LLMs for Multi-Hop Reasoning - Neo4j](https://neo4j.com/blog/genai/knowledge-graph-llm-multi-hop-reasoning/)
[MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries - OpenReview](https://openreview.net/forum?id=t4eB3zYWBK)
[Agentic RAG with Knowledge Graphs for Complex Multi-Hop Reasoning in Real-World Applications - arXiv](https://arxiv.org/html/2507.16507v1)
[MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries - Medium](https://medium.com/@EleventhHourEnthusiast/multihop-rag-benchmarking-retrieval-augmented-generation-for-multi-hop-queries-ebff47e8a401)
[HopRAG: A Logic-driven RAG for Multi-hop Question Answering - arXiv](https://arxiv.org/html/2502.12442v1)
[LevelRAG: A High-Level Framework for Large Language Model-based Agents in RAG - arXiv](https://arxiv.org/html/2502.18139v1)
[Agentic RAG - LangGraph](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/)
[Understanding Graph-based RAG and Multi-hop Question Answering - Zyphra](https://www.zyphra.com/post/understanding-graph-based-rag-and-multi-hop-question-answering)
[Retrieval-augmented generation (RAG) - LangChain](https://python.langchain.com/docs/tutorials/rag/)
[BGE Reranker V2 - BGE Model](https://bge-model.com/bge/bge_reranker_v2.html)
[Reranking - BGE Model](https://bge-model.com/tutorial/5_Reranking/5.2.html)
[Discussion on bge-reranker-v2-m3 - Hugging Face](https://huggingface.co/BAAI/bge-reranker-v2-m3/discussions/22)
[bge-reranker-v2-m3-onnx-o3-cpu - PromptLayer](https://www.promptlayer.com/models/bge-reranker-v2-m3-onnx-o3-cpu)
[Rerankers in RAG: An In-depth Exploration of Architectures, Loss Functions, and Training Strategies - arXiv](https://arxiv.org/html/2409.07691v1)
[Query Decomposition - Haystack](https://haystack.deepset.ai/cookbook/query_decomposition)
[MA-RAG: A Multi-Agent Framework for Retrieval Augmented Generation - arXiv](https://arxiv.org/html/2505.20096v1)
[Query Decomposition for RAG - Haystack](https://haystack.deepset.ai/blog/query-decomposition)
[Agentic RAG with Knowledge Graphs for Complex Multi-Hop Reasoning in Real-World Applications - ResearchGate](https://www.researchgate.net/publication/393923781_Agentic_RAG_with_Knowledge_Graphs_for_Complex_Multi-Hop_Reasoning_in_Real-World_Applications)
[Multi-Agent RAG with Interleaved Retrieval and Reasoning - Pathway](https://pathway.com/blog/multi-agent-rag-interleaved-retrieval-reasoning/)
[PAR-RAG: A Plan-Driven Approach for Retrieval Augmentation in Multi-Hop Question Answering - arXiv](https://arxiv.org/html/2504.16787)
[UnstructuredPDFLoader - LangChain](https://python.langchain.com/docs/integrations/document_loaders/unstructured_pdfloader/)
[PDF - LangChain](https://python.langchain.com/docs/how_to/document_loader_pdf/)
[Unstructured File - LangChain](https://python.langchain.com/docs/integrations/document_loaders/unstructured_file/)
[langchain_community.document_loaders.pdf.UnstructuredPDFLoader - LangChain](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.UnstructuredPDFLoader.html)
[Best PDF parser for RAG? - Reddit](https://www.reddit.com/r/LangChain/comments/1dzj5qx/best_pdf_parser_for_rag/)
[langchain_community.document_loaders.pdf.UnstructuredPDFLoader - LangChain](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.UnstructuredPDFLoader.html)
[UnstructuredPDFLoader - LangChain](https://docs.langchain.com/oss/python/integrations/document_loaders/unstructured_pdfloader)
[PDF - LangChain.js](https://js.langchain.com/docs/integrations/document_loaders/file_loaders/pdf/)
[Document loaders - LangChain](https://python.langchain.com/docs/integrations/document_loaders/)
[How can I parse unstructured document? - Reddit](https://www.reddit.com/r/LangChain/comments/1eyyv3d/how_can_i_parse_unstructured_document/)
[Hybrid Search with Postgres Native BM25 and VectorChord - VectorChord](https://blog.vectorchord.ai/hybrid-search-with-postgres-native-bm25-and-vectorchord)
[pgvector/pgvector - GitHub](https://github.com/pgvector/pgvector)
[Hybrid Search with Postgres - Reddit](https://www.reddit.com/r/LangChain/comments/1dvdnzc/hybrid_search_with_postgres/)
[Hybrid Search - VectorChord](https://docs.vectorchord.ai/vectorchord/use-case/hybrid-search.html)
[Hybrid Search With PostgreSQL and Pgvector - Jonathan S. Katz](https://jkatz05.com/post/postgres/hybrid-search-postgres-pgvector/)
[Stop the Hallucinations: Hybrid Retrieval with BM25, pgvector, Embedding, Rerank, LLM Rubric Rerank - Medium](https://medium.com/@richardhightower/stop-the-hallucinations-hybrid-retrieval-with-bm25-pgvector-embedding-rerank-llm-rubric-rerank-895d8f7c7242)
[Postgres as a Vector Database: Implementing Hybrid Search with Postgres for RAG using Groq - Medium](https://medium.com/@meeran03/postgres-as-a-vector-database-implementing-hybrid-search-with-postgres-for-rag-using-groq-494ca3e41d57)
[Combining Semantic Search and Full-Text Search in PostgreSQL with Cohere, pgvector, and pgai - Tiger Analytics](https://www.tigerdata.com/blog/combining-semantic-search-and-full-text-search-in-postgresql-with-cohere-pgvector-and-pgai)
[tensorchord/VectorChord-bm25 - GitHub](https://github.com/tensorchord/VectorChord-bm25)
[Hybrid search - Supabase](https://supabase.com/docs/guides/ai/hybrid-search)
[Advanced Parsing - tree-sitter](https://tree-sitter.github.io/tree-sitter/using-parsers/3-advanced-parsing.html)
[Getting Started - tree-sitter](https://tree-sitter.github.io/tree-sitter/creating-parsers/1-getting-started.html)
[Creating Parsers - tree-sitter](https://tree-sitter.github.io/tree-sitter/creating-parsers/)
[Using Parsers - tree-sitter](https://tree-sitter.github.io/tree-sitter/using-parsers/)
[CintraAI/code-chunker - GitHub](https://github.com/CintraAI/code-chunker)
[tree-sitter/tree-sitter - GitHub](https://github.com/tree-sitter/tree-sitter)
[tree-sitter - GitHub](https://github.com/tree-sitter)
[Tree-sitter (parser generator) - Wikipedia](https://en.wikipedia.org/wiki/Tree-sitter_(parser_generator))
[Tree-sitter Advanced - Haobo Gu's Blog](https://haobogu.github.io/posts/code-intelligence/tree-sitter-advanced/)
[Pgvector vs. Qdrant - Tiger Analytics](https://www.tigerdata.com/blog/pgvector-vs-qdrant)
[timescale/pgvectorscale - GitHub](https://github.com/timescale/pgvectorscale)
[Pgvector vs. Pinecone - Tiger Analytics](https://www.tigerdata.com/blog/pgvector-vs-pinecone)
[Optimizing Vector Search at Scale: Lessons from pgvector & Supabase Performance Tuning - Medium](https://medium.com/@dikhyantkrishnadalai/optimizing-vector-search-at-scale-lessons-from-pgvector-supabase-performance-tuning-ce4ada4ba2ed)
[Supercharging vector search performance and relevance with pgvector 0.8.0 on Amazon Aurora PostgreSQL - AWS Database Blog](https://aws.amazon.com/blogs/database/supercharging-vector-search-performance-and-relevance-with-pgvector-0-8-0-on-amazon-aurora-postgresql/)
[Pinecone vs. pgvector - Pinecone](https://www.pinecone.io/blog/pinecone-vs-pgvector/)
[Optimize performance with pgvector on Azure Cosmos DB for PostgreSQL - Microsoft Learn](https://learn.microsoft.com/en-us/azure/cosmos-db/postgresql/howto-optimize-performance-pgvector)
[Postgres Vector Search with pgvector: Benchmarks, Costs, and Reality Check - Medium](https://medium.com/@DataCraft-Innovations/postgres-vector-search-with-pgvector-benchmarks-costs-and-reality-check-f839a4d2b66f)
[pgvector performance: 150x speedup with HNSW and binary quantization - Jonathan S. Katz](https://jkatz05.com/post/postgres/pgvector-performance-150x-speedup/)
[Hybrid Search - Qdrant](https://qdrant.tech/articles/hybrid-search/)
[Hybrid Search Explained - Weaviate](https://weaviate.io/blog/hybrid-search-explained)
[Optimizing RAG with Hybrid Search & Reranking - Superlinked](https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking)
[Hybrid Search: Combining BM25 and Semantic Search for Better Results with Langchain and LanceDB - Medium](https://medium.com/etoai/hybrid-search-combining-bm25-and-semantic-search-for-better-results-with-lan-1358038fe7e6)
[Hybrid Vigor: Winning at Hybrid Search - OpenSource Connections](https://opensourceconnections.com/blog/2023/02/27/hybrid-vigor-winning-at-hybrid-search/)
[How do I implement BM25 alongside vector search? - Milvus](https://milvus.io/ai-quick-reference/how-do-i-implement-bm25-alongside-vector-search)
[BM25Retriever - LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/retrievers/bm25_retriever/)
[What is Hybrid Search? - Elastic](https://www.elastic.co/what-is/hybrid-search)