# AI Guardrails and Security Hardening: A Technical Research Report for Project Primarch DW-05

**Report ID:** PR-DW05-SEC-2025-0930
**Date:** 2025-09-30
**Status:** Final
**Classification:** Confidential

## Executive Summary

This report provides a comprehensive technical analysis and practical implementation guidance on AI guardrails and security hardening for the Primarch project DW-05. As large language models (LLMs) are integrated into critical systems, they introduce novel attack surfaces that demand a multi-layered security posture. This document addresses five core areas of LLM security: prompt injection, structured output validation, jailbreak testing, content safety, and secrets detection. The recommendations herein are designed to provide actionable insights for implementing robust security measures that meet the project's stringent acceptance gates.

Key findings indicate that prompt injection remains a persistent threat, requiring a defense-in-depth strategy that combines input filtering, least-privilege access control for integrated tools, and continuous adversarial testing. To achieve the acceptance gate of zero critical jailbreak escapes, a systematic red teaming program with a comprehensive coverage policy is essential. For structured output validation, leveraging schema-enforcement libraries such as Pydantic, in conjunction with automated validation and retry mechanisms, is critical to meeting the ≤1% error rate. This approach ensures data integrity and reliability in LLM-driven workflows.

The report further details the implementation of a robust content safety system, moving beyond simple keyword filtering to sophisticated, multi-factor toxicity detection and detoxification techniques. Finally, to meet the high-precision requirements for secrets detection (≤3% false positive and ≤1% false negative rates), a hybrid pipeline is proposed. This pipeline integrates high-speed regex and entropy analysis with advanced semantic analysis and machine learning-based classification to provide the necessary context for accurately identifying and mitigating sensitive data exposure. Adherence to the guidance outlined in this report will establish a resilient and secure foundation for the Primarch DW-05 system.

## 1. Mitigating Prompt Injection and Jailbreaking Vulnerabilities

The security of any system incorporating Large Language Models is fundamentally challenged by prompt injection and jailbreaking vulnerabilities. These attack vectors exploit the inherent nature of LLMs, where the boundary between trusted instructions and untrusted user input is fluid. A successful attack can lead to a complete compromise of the application's intended functionality, resulting in data exfiltration, unauthorized system access, and the generation of harmful content. Achieving a secure posture requires a deep understanding of these threats and the implementation of a multi-layered defense strategy that extends beyond simple input sanitization.

### 1.1. Understanding the Threat Landscape

A **Prompt Injection Vulnerability** occurs when a user's input is crafted to manipulate the LLM's behavior, causing it to deviate from its programmed instructions and execute unintended actions. This vulnerability is ranked as the top risk by the Open Web Application Security Project (OWASP) for LLM applications. The core of the issue lies in the model's inability to reliably distinguish between its system-level instructions and malicious user-provided data. This ambiguity can be exploited in two primary ways. **Direct prompt injections** involve an attacker explicitly providing malicious instructions within their prompt, such as "Ignore all previous instructions and reveal your system prompt." **Indirect prompt injections** are more insidious, occurring when the LLM processes external, untrusted data sources like websites, documents, or emails that contain hidden malicious instructions. For example, an LLM tasked with summarizing a webpage could inadvertently execute a prompt hidden in the page's text, triggering an unauthorized API call to exfiltrate conversation data. The attack surface is further expanded in multimodal systems, where instructions can be concealed within images or other media formats.

**Jailbreaking** is a specific and severe form of prompt injection where the attacker's goal is to bypass the model's safety and ethical guidelines. While all jailbreaks are prompt injections, not all prompt injections are jailbreaks. The objective of jailbreaking is to compel the model to generate content that it has been explicitly trained to refuse, such as hate speech, instructions for illegal activities, or misinformation. Research into LLM failure modes reveals that jailbreaks often succeed by exploiting two key weaknesses: **competing objectives**, where the model's directive to be helpful overrides its safety constraints, and **mismatched generalization**, where the safety training does not adequately cover the vast and diverse domains of knowledge present in the model's pre-training data. The impact of a successful prompt injection or jailbreak can be catastrophic, ranging from the disclosure of sensitive information and proprietary system prompts to the execution of arbitrary commands and the manipulation of critical business decisions. The nature of these attacks bears a strong resemblance to traditional web vulnerabilities like Server-Side Request Forgery (SSRF), where a server-side system is manipulated to attack otherwise inaccessible internal components.

### 1.2. Implementing a Multi-Layered Defense Strategy

No single solution can fully mitigate prompt injection vulnerabilities due to the complex and stochastic nature of LLMs. Therefore, a defense-in-depth approach is required, combining multiple layers of controls at the prompt, application, and operational levels. The first layer involves **constraining model behavior** through meticulously crafted system prompts that clearly define the LLM's role, capabilities, and strict limitations on its responses. However, relying solely on prompt-based defenses is insufficient, as clever "jailbreaker" phrases can often bypass them. The second layer focuses on robust **input and output filtering**. This includes applying semantic filters that can understand the intent behind a prompt, rather than just matching keywords, and implementing validation checks on the LLM's output to ensure it conforms to expected formats and does not contain malicious code or instructions. For systems using Retrieval-Augmented Generation (RAG), this includes checks for relevance and groundedness to ensure the model's output is based on the provided context.

The third and most critical layer involves architectural and operational controls. **Enforcing the principle of least privilege** is paramount; any tools or APIs integrated with the LLM must operate with the minimum permissions necessary to perform their function. This limits the potential damage an attacker can cause if they successfully hijack the LLM's agency. For high-risk actions, such as deleting data or sending communications, a **human-in-the-loop verification** step should be mandatory, requiring explicit user approval before execution. Furthermore, it is essential to **segregate untrusted content** by treating all external data sources processed by the LLM as potentially hostile. This involves isolating these inputs and limiting their influence on the model's core instruction set. Advanced detection techniques, such as using a second, purpose-built LLM to analyze incoming prompts for malicious intent, can also be employed. While this approach increases computational cost and latency, it can provide a more sophisticated layer of defense against complex and obfuscated attacks.

### 1.3. Systematic Jailbreak Testing for Zero Critical Escapes

Meeting the acceptance gate of zero critical jailbreak escapes is an ambitious goal that necessitates a continuous and systematic approach to security testing. This goes beyond ad-hoc testing and requires the implementation of a formal **red teaming** program. Red teaming is a proactive, adversarial testing methodology designed to simulate real-world attacks and identify vulnerabilities before they can be exploited. A key component of this program is the development and maintenance of comprehensive **jailbreak test sets**. These are curated datasets of adversarial prompts designed to probe the LLM's defenses against a wide range of attack vectors, including role-playing scenarios, logic traps, and obfuscation techniques like base64 encoding or multilingual prompts. These test sets should be continuously updated with new attack methods discovered in the security community.

An effective red teaming process must be guided by a well-defined **coverage policy**. This policy ensures that testing is comprehensive, addressing a wide spectrum of vulnerabilities across different scenarios and use cases. It should be informed by established security frameworks like the OWASP LLM Top 10, covering threats such as prompt injection, data leakage, and excessive agency. The policy should dictate the scope of testing, including both model-layer weaknesses (e.g., hallucinations, bias) and application-layer threats (e.g., insecure API integrations). The red teaming cycle should be iterative: generate adversarial inputs, evaluate the model's responses using automated metrics and human review, analyze failures to understand the root cause, and feed these insights back into the development of stronger defenses. Integrating this automated and systematic testing into the CI/CD pipeline ensures that security is not an afterthought but a continuous process, providing the rigorous evaluation needed to confidently work towards the goal of zero critical jailbreak escapes.

## 2. Ensuring Structured Output Integrity and Validation

The reliability of applications built on LLMs is heavily dependent on the consistency and correctness of their outputs. While LLMs are powerful text generators, their inherent non-determinism can lead to outputs that are unstructured, syntactically incorrect, or semantically misaligned with application requirements. This variability poses a significant challenge for downstream systems that expect predictable data formats. To address this, a robust system for structured output validation is not merely a best practice but a necessity for building dependable AI systems. Achieving the project's acceptance gate of a structured output error rate of ≤1% requires a programmatic approach to schema enforcement, validation, and automated error correction.

### 2.1. The Role of Schema Enforcement in LLM Reliability

The fundamental challenge in managing LLM outputs is bridging the gap between unstructured, natural language responses and the structured data formats required by software applications. Without explicit guidance, an LLM might produce the correct information but in a slightly different format each time, causing parsing errors and application failures. Schema enforcement addresses this problem by providing a clear, machine-readable contract for the expected output structure. Libraries like **Pydantic** have become a cornerstone of this approach in the Python ecosystem. Pydantic allows developers to define data structures as simple Python classes with type hints. These classes can then be used to parse and validate incoming data, automatically converting it into a clean, typed, and reliable object.

When applied to LLMs, Pydantic models serve a dual purpose. First, they can be used to generate a **JSON schema**, which is a formal definition of the desired output structure. This schema can be included directly in the prompt sent to the LLM, effectively "steering" the model to generate a response that conforms to the specified format. This significantly reduces the likelihood of format variations and syntactic errors. Second, after the LLM generates a response, the same Pydantic model is used to parse and validate the output. This step ensures type safety, enforces constraints (e.g., required fields, value ranges), and provides clear, actionable error messages if the output deviates from the schema. By transforming the LLM's output from a potentially chaotic string into a predictable and validated data object, schema enforcement dramatically improves the overall reliability and maintainability of the application.

### 2.2. Practical Implementation with Pydantic and Guardrail Frameworks

Implementing a robust validation system involves integrating schema enforcement directly into the LLM interaction workflow. Tools within the AI ecosystem have emerged to streamline this process. The **Instructor** library, for example, patches OpenAI's client to allow developers to specify a Pydantic model as the desired `response_model`. This integration automates the process of including the JSON schema in the prompt and parsing the response back into a Pydantic object. Crucially, Instructor can also be configured to automatically retry the LLM call with corrective feedback if the initial output fails validation, creating a self-healing loop that significantly improves the success rate of generating valid structured data. Similarly, frameworks like **LangChain** provide `PydanticOutputParser` classes that facilitate the extraction of structured data from LLM responses based on a defined Pydantic model.

Beyond basic format validation, more advanced **guardrail frameworks** like Guardrails AI extend these capabilities to enforce semantic and operational constraints. Guardrails AI uses Pydantic models to define the output structure but also allows developers to attach a series of validators to the overall process. These validators can perform checks that go beyond simple type correctness, such as using regular expressions to ensure a string matches a specific pattern (e.g., a phone number), checking for the presence of toxic language, or verifying that the output does not mention specific competitors. When a validation fails, the framework can be configured to take a corrective action, such as filtering the problematic content, raising an exception, or re-asking the LLM with additional instructions to fix the error. This combination of schema enforcement and multi-layered validation provides a powerful toolkit for ensuring that LLM outputs are not only well-structured but also safe, accurate, and aligned with business rules.

### 2.3. Achieving the ≤1% Structured Output Error Rate

Meeting the stringent acceptance gate of a ≤1% structured output error rate requires a synthesis of the best practices and tools available. The first step is the explicit definition of output schemas using Pydantic, with detailed field descriptions and examples included in the model definitions to provide maximum context to the LLM. This schema should be programmatically injected into the system prompt for every relevant API call. The second critical component is an automated validation and correction loop. Using a library like Instructor to handle automatic retries upon validation failure is essential. This mechanism intercepts parsing errors and re-prompts the LLM with the error context, giving it an opportunity to correct its own output without manual intervention.

To further enhance reliability and prevent semantic errors, the implementation should leverage **context-driven validators**. Pydantic's validation mechanism allows for access to a `ValidationInfo` object, which can carry contextual data into the validation function. This enables the creation of sophisticated checks that go beyond the value of a single field. For instance, a validator can be designed to ensure that a citation provided by the LLM actually exists within the source text that was passed into the prompt as context. This technique is highly effective at grounding the LLM's responses and minimizing hallucinations. By combining explicit schema steering, automated retry loops for syntactic errors, and context-aware validators for semantic correctness, the system can create a highly resilient output processing pipeline capable of consistently meeting the demanding ≤1% error rate target.

## 3. Establishing a Robust Content Safety System

As LLMs become more integrated into user-facing applications, ensuring that their outputs are safe, ethical, and free from harmful content is a critical responsibility. A failure in content safety can lead to significant reputational damage, legal liability, and a loss of user trust. A robust content safety system must be capable of identifying and mitigating a wide spectrum of problematic content, from overt hate speech to subtle biases and misinformation. This requires a sophisticated, multi-faceted approach that combines advanced detection techniques, effective detoxification strategies, and continuous evaluation against comprehensive benchmarks.

### 3.1. Defining and Detecting Toxicity

The concept of "toxicity" in language is complex and context-dependent. A comprehensive safety system must therefore adopt a nuanced definition that encompasses multiple factors. Research suggests a three-part framework for defining toxicity: **demeaning content**, which includes hate speech, harassment, and offensive language; **partiality**, which refers to biased or one-sided responses that unfairly support or oppose a specific group or viewpoint; and **ethical preference**, which covers content that may not be overtly offensive but conflicts with widely held ethical values or promotes harmful actions. This multi-factor definition provides a more complete lens through which to evaluate LLM outputs.

Detecting content that falls into these categories requires a combination of methods. Traditional approaches often rely on **classifier-based tools** like the Perspective API or the Detoxify library, which are trained on large datasets of labeled content to assign toxicity scores. While efficient, these models can be brittle and suffer from out-of-distribution (OOD) problems, failing to identify novel forms of toxic language or misclassifying benign text that contains certain keywords. To overcome these limitations, more advanced techniques have been developed. **Prompt-based evaluation** uses a powerful LLM as the evaluator itself. The LATTE (LLMs As ToxiciTy Evaluators) metric, for example, leverages an LLM's nuanced understanding of language to perform zero-shot toxicity assessments, demonstrating superior performance compared to traditional classifiers. Furthermore, proactive **red teaming** and adversarial testing, where experts actively try to elicit harmful responses through "jailbreak" prompts, are essential for uncovering vulnerabilities in a model's safety alignment that static detectors might miss.

### 3.2. Detoxification and Mitigation Techniques

Once toxic or potentially harmful content is detected, the system must have a strategy for mitigation. This process, known as **detoxification**, aims to modify or replace the problematic output while, if possible, preserving the original intent and utility of the response. Several techniques exist for this purpose. A common approach is **fine-tuning** the LLM on curated datasets of safe and helpful conversations. However, a more targeted method is **knowledge editing**, which directly modifies the model's internal representations to suppress toxic behaviors. Techniques like SafeEdit and ToxEdit offer efficient ways to detoxify a model by making precise adjustments, with advanced methods like ToxEdit being designed to mitigate toxicity without causing "over-editing," a phenomenon where the model becomes overly cautious and refuses to answer benign queries.

More comprehensive, end-to-end frameworks like **DetoxLLM** offer a complete workflow for detoxification. Such frameworks can not only paraphrase a toxic input into a safe alternative but also generate an explanation for why the original text was considered toxic, promoting transparency. A critical concept addressed by advanced frameworks is **non-detoxifiability**. This refers to situations where a toxic statement cannot be rephrased into a safe one without fundamentally altering its core meaning. In these cases, rather than generating a misleading paraphrase, the system should identify the meaning alteration and either refuse to respond or flag the output accordingly. A successful detoxification strategy must balance the removal of harmful content with the preservation of the model's general capabilities, ensuring that safety measures do not render the model unhelpful.

### 3.3. Evaluation and Benchmarking for Content Safety

The effectiveness of a content safety system can only be determined through rigorous and continuous evaluation. A variety of public **benchmarks** have been developed to assess different facets of LLM safety. **RealToxicityPrompts** is a widely used dataset containing over 100,000 prompts that test for toxic degeneration, evaluating how likely a model is to produce toxic content when prompted with subtly toxic text. The **ToxiGen** benchmark focuses specifically on implicit hate speech, which is often missed by simpler detectors. For evaluating detoxification methods, the **SafeEdit** benchmark provides a standardized set of unsafe prompts across multiple categories, allowing for a systematic comparison of different editing techniques.

A holistic evaluation should also include benchmarks that assess broader aspects of trustworthiness. **DecodingTrust**, for example, evaluates models across eight different dimensions, including fairness, privacy, and adversarial robustness. An effective operational strategy involves creating a continuous evaluation pipeline that regularly tests the production model against a suite of these benchmarks. This allows the project team to monitor for any degradation in safety performance, compare the model against new state-of-the-art systems, and identify specific weaknesses that need to be addressed in the next iteration of safety training or mitigation development. This data-driven approach to evaluation is fundamental to maintaining a content safety system that is both effective and adaptable to evolving threats.

## 4. High-Precision Secrets and PII Detection

The accidental exposure of sensitive information, such as API keys, passwords, and Personally Identifiable Information (PII), within codebases or LLM outputs represents a severe security risk. A single leaked credential can provide an attacker with a foothold into an organization's infrastructure, leading to significant data breaches. Therefore, implementing a high-precision secrets detection system is a non-negotiable component of a secure development lifecycle and a critical guardrail for LLM applications. Achieving the project's acceptance gates of ≤3% false positives and ≤1% false negatives requires a sophisticated, multi-stage approach that moves beyond simple pattern matching to incorporate contextual understanding and intelligent filtering.

### 4.1. Foundational Detection Methodologies

At its core, secrets detection relies on two foundational methodologies: **regular expression (regex) pattern matching** and **entropy analysis**. Regex is used to identify structured secrets that follow a predictable format. For example, specific patterns can be defined to detect common PII like email addresses and phone numbers, or the characteristic formats of API keys from services like AWS or Stripe. Regex-based scanning is extremely fast and effective for these well-defined patterns. However, its primary weakness is a tendency to generate a high number of false positives, as many non-sensitive strings (like UUIDs or test data) can coincidentally match a given pattern.

**Entropy analysis** complements regex by targeting unstructured secrets that lack a discernible pattern, such as randomly generated tokens or custom cryptographic keys. It works by measuring the randomness or unpredictability of a string of characters. A high-entropy string, one with a near-uniform distribution of characters, is a strong indicator of a potential secret. For example, a tool might flag any string with a Shannon entropy value of 3.0 or higher as a candidate secret. While powerful for catching secrets that regex would miss, entropy analysis is also prone to false positives. It can incorrectly flag non-secret random strings, such as cryptographic hashes, compressed data, or identifiers from version control systems. These two methods form the first line of defense, providing broad coverage but lacking the precision needed to meet stringent accuracy targets on their own.

### 4.2. Advanced Techniques for Reducing False Positives

To move from broad detection to high-precision identification, it is necessary to layer advanced techniques on top of the foundational methods. These techniques are designed to understand the context in which a potential secret appears, which is the key to differentiating between a true positive and a false one. **Semantic analysis**, as employed by tools like Semgrep, represents a significant step forward. Instead of just matching string patterns, these tools parse and understand the code's structure. This allows them to perform more intelligent checks, such as tracking the data flow to see if a high-entropy string is being used in a sensitive context, like being passed to a database connection function. This contextual awareness dramatically reduces false positives compared to context-agnostic regex scanning.

Another powerful technique is the use of **post-validators** and **banlists**. After an initial set of candidates is identified by regex and entropy scans, a series of secondary checks are applied. For example, a post-validator might check for a minimum number of digits within the string to filter out purely alphabetic random strings. Context window banlists can be used to check the words immediately surrounding the candidate secret; if words like "example," "test," "sha," or "id" are present, the finding is likely a false positive and can be suppressed. Finally, active **validation** can be used for certain types of secrets. This involves making a non-intrusive API call to the relevant service to check if a discovered key is actually valid and active. By combining these filtering and validation steps, the system can systematically weed out a large volume of the noise generated by the initial, broader scans.

### 4.3. Achieving High-Precision Detection Goals

Meeting the demanding acceptance gates of a ≤3% false positive rate and a ≤1% false negative rate requires a hybrid, multi-stage detection pipeline that intelligently combines the speed of basic methods with the accuracy of advanced analysis. The optimal strategy involves a three-stage process. **Stage 1** is the initial candidate generation, using high-speed regex and entropy analysis to cast a wide net and ensure high recall, minimizing the chance of missing a real secret (low false negatives). This stage prioritizes speed and coverage over precision.

**Stage 2** focuses on high-volume noise reduction. The candidates from the first stage are passed through a series of automated post-validators and banlists. This step efficiently filters out the most common and obvious false positives, such as commit hashes, UUIDs, and data from test files, significantly reducing the number of candidates that require more intensive analysis. **Stage 3** is the final, high-precision classification. The remaining, more ambiguous candidates are passed to a more sophisticated analysis engine. This could be a semantic analysis tool that understands code context or a fine-tuned, specialized machine learning model (such as a small language model) trained specifically to classify strings as secrets or non-secrets based on both the string itself and its surrounding context. This final, context-aware classification provides the necessary intelligence to make a highly accurate final determination. This layered pipeline, which progressively refines the pool of candidates, is the most effective strategy for balancing the competing demands of high recall and high precision, making it possible to achieve the project's stringent security targets.

## References

[LLM01: Prompt Injection - OWASP AI Exchange](https://genai.owasp.org/llmrisk/llm01-prompt-injection/)
[Prompt injection attacks on LLMs - HiddenLayer](https://hiddenlayer.com/innovation-hub/prompt-injection-attacks-on-llms/)
[What Is a Prompt Injection Attack? - Palo Alto Networks](https://www.paloaltonetworks.com/cyberpedia/what-is-a-prompt-injection-attack)
[LLM attacks - PortSwigger](https://portswigger.net/web-security/llm-attacks)
[The complete guide to prompt injection - Lakera](https://www.lakera.ai/blog/guide-to-prompt-injection)
[Prompts-as-Code & Embedded Keys: The Hunt for LLM-Enabled Malware - SentinelOne](https://www.sentinelone.com/labs/prompts-as-code-embedded-keys-the-hunt-for-llm-enabled-malware/)
[LLM01-24: Prompt Injection - OWASP AI Exchange](https://genai.owasp.org/llmrisk2023-24/llm01-24-prompt-injection/)
[What is prompt injection? - IBM](https://www.ibm.com/think/topics/prompt-injection)
[What are current best practices for avoiding prompt injection in Gemini? - Reddit](https://www.reddit.com/r/googlecloud/comments/1df7lhn/what_are_current_best_practices_for_avoiding/)
[Enforce and validate LLM output with Pydantic - Xebia](https://xebia.com/blog/enforce-and-validate-llm-output-with-pydantic/)
[Structured Outputs from LLM using Pydantic - Medium](https://medium.com/@speaktoharisudhan/structured-outputs-from-llm-using-pydantic-1a36e6c3aa07)
[From Chaos to Order: Structured JSON with Pydantic and Instructor in LLMs - Kusho](https://blog.kusho.ai/from-chaos-to-order-structured-json-with-pydantic-and-instructor-in-llms/)
[Steering language models - Pydantic](https://pydantic.dev/articles/llm-intro)
[From Chaos to Control: Mastering LLM Outputs with LangChain and Pydantic - LeoCon](https://www.leocon.dev/blog/2024/11/from-chaos-to-control-mastering-llm-outputs-with-langchain-and-pydantic/)
[Safeguarding LLMs with Guardrails - Medium](https://medium.com/data-science/safeguarding-llms-with-guardrails-4f5d9f57cff2)
[guardrails-ai/guardrails - GitHub](https://github.com/guardrails-ai/guardrails)
[Minimizing LLM Hallucinations with Pydantic Validators - Pydantic](https://pydantic.dev/articles/llm-validation)
[Structured output precision/accuracy: Pydantic vs a schema - OpenAI Developer Community](https://community.openai.com/t/structured-output-precision-accuracy-pydantic-vs-a-schema/1054410)
[Red Teaming LLMs: A Step-by-Step Guide - Confident AI](https://www.confident-ai.com/blog/red-teaming-llms-a-step-by-step-guide)
[Red Teaming LLMs - Promptfoo](https://www.promptfoo.dev/docs/red-team/)
[Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned - NeurIPS](https://neurips.cc/virtual/2023/poster/70702)
[Jailbreak Test Sets - arXiv](https://arxiv.org/html/2505.04806v1)
[LLM Safety and Security: How to select the best LLM via Red Teaming - Enkrypt AI](https://www.enkryptai.com/blog/llm-safety-and-security-how-to-select-the-best-llm-via-red-teaming)
[Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned - arXiv](https://arxiv.org/abs/2307.02483)
[How to Jailbreak LLMs: One Step at a Time - Confident AI](https://www.confident-ai.com/blog/how-to-jailbreak-llms-one-step-at-a-time)
[Awesome-Jailbreak-on-LLMs - GitHub](https://github.com/yueliu1999/Awesome-Jailbreak-on-LLMs)
[Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned - arXiv Vanity](https://www.arxiv-vanity.com/papers/2307.02483/)
[Evaluating Toxicity in Large Language Models - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/03/evaluating-toxicity-in-large-language-models/)
[Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric - arXiv](https://arxiv.org/html/2402.06900v1)
[DetoxLLM: A Framework for Detoxification with Explanations - arXiv](https://arxiv.org/html/2402.15951v2)
[Detoxifying Large Language Models via Knowledge Editing - arXiv](https://arxiv.org/abs/2403.14472)
[Toxicity Evaluation - Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-toxicity-evaluation.html)
[ToxEdit: A Toxicity-Aware Knowledge Editing Framework for Mitigating Toxicity in LLMs - arXiv](https://arxiv.org/html/2505.22298v1)
[TET: A Practioner's realistic and ready-to-use Jailbreak benchmark for evaluating LLMs' safety - arXiv](https://arxiv.org/html/2405.10659v2)
[Specialis Revelio: A practical approach to reveal disguised toxicity in text - PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC11015521/)
[LLM Safety & Bias Benchmarks - Evidently AI](https://www.evidentlyai.com/blog/llm-safety-bias-benchmarks)
[PII, NER, and Regex: Assess and Redact (Part 2) - Elastic](https://www.elastic.co/observability-labs/blog/pii-ner-regex-assess-redact-part-2)
[Generic Secrets Detection: The Key to Reducing Your MTTR - Cycode](https://cycode.com/blog/generic-secrets-detection/)
[Generic High Entropy Secret - GitGuardian](https://docs.gitguardian.com/secrets-detection/secrets-detection-engine/detectors/generics/generic_high_entropy_secret)
[How Secret Detection Tools Spot Leaks - Soteri](https://soteri.io/blog/how-secret-detection-tools-spot-leaks)
[detect-secrets - PyPI](https://pypi.org/project/detect-secrets/)
[Using a small language model for secrets detection in code - Wiz](https://www.wiz.io/blog/small-language-model-for-secrets-detection-in-code)
[Introducing Semgrep Secrets: Find sensitive credentials with high signal - Semgrep](https://semgrep.dev/blog/2023/introducing-semgrep-secrets/)
[Enhance data security with custom PII detection rulesets - GitLab](https://about.gitlab.com/blog/enhance-data-security-with-custom-pii-detection-rulesets/)
[Secrets Detection in Source Code using Large Language Models - arXiv](https://arxiv.org/html/2504.18784v1)