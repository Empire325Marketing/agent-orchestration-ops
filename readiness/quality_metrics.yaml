# RAG 2.0 Quality Metrics and Thresholds
# Defines acceptance criteria, performance gates, and quality standards

version: "2.0"
last_updated: "2025-09-30"

# ======================
# RETRIEVAL QUALITY THRESHOLDS
# ======================

retrieval_quality:
  # Core retrieval metrics
  recall_at_k:
    recall_at_5:
      minimum: 0.70
      target: 0.80
      excellent: 0.85
    recall_at_10:
      minimum: 0.75
      target: 0.85
      excellent: 0.90
    recall_at_20:
      minimum: 0.80
      target: 0.90
      excellent: 0.95

  # Normalized Discounted Cumulative Gain
  ndcg:
    ndcg_at_5:
      minimum: 0.72
      target: 0.80
      excellent: 0.87
    ndcg_at_10:
      minimum: 0.75
      target: 0.83
      excellent: 0.90
    ndcg_at_20:
      minimum: 0.78
      target: 0.85
      excellent: 0.92

  # Mean Reciprocal Rank
  mrr:
    minimum: 0.70
    target: 0.79
    excellent: 0.85
    
  # Context Precision (relevance of retrieved contexts)
  context_precision:
    minimum: 0.75
    target: 0.85
    excellent: 0.92

  # Context Recall (coverage of relevant information)
  context_recall:
    minimum: 0.80
    target: 0.90
    excellent: 0.95

# ======================
# RERANKING QUALITY THRESHOLDS
# ======================

reranking_quality:
  # Improvement metrics (vs retrieval-only)
  quality_improvement:
    ndcg_improvement:
      minimum: 0.10  # 10% improvement
      target: 0.15   # 15% improvement
      excellent: 0.25 # 25% improvement
    mrr_improvement:
      minimum: 0.08  # 8% improvement
      target: 0.12   # 12% improvement
      excellent: 0.20 # 20% improvement

  # Reranking accuracy
  rerank_accuracy:
    top_1_accuracy:
      minimum: 0.75
      target: 0.85
      excellent: 0.92
    top_3_accuracy:
      minimum: 0.85
      target: 0.92
      excellent: 0.97

  # Ranking correlation with ground truth
  ranking_correlation:
    spearman_correlation:
      minimum: 0.70
      target: 0.80
      excellent: 0.90
    kendall_tau:
      minimum: 0.65
      target: 0.75
      excellent: 0.85

# ======================
# PERFORMANCE THRESHOLDS
# ======================

performance_metrics:
  # Latency requirements
  latency:
    # Embedding generation
    embedding_latency:
      p50_ms:
        maximum: 50
        target: 30
        excellent: 20
      p95_ms:
        maximum: 100
        target: 60
        excellent: 40
      p99_ms:
        maximum: 200
        target: 120
        excellent: 80

    # Vector search
    search_latency:
      p50_ms:
        maximum: 30
        target: 20
        excellent: 15
      p95_ms:
        maximum: 80
        target: 50
        excellent: 30
      p99_ms:
        maximum: 150
        target: 100
        excellent: 60

    # Reranking latency (critical requirement: â‰¤200ms p95)
    rerank_latency:
      p50_ms:
        maximum: 25
        target: 15
        excellent: 10
      p95_ms:
        maximum: 200  # Hard requirement
        target: 50
        excellent: 35
      p99_ms:
        maximum: 500
        target: 100
        excellent: 80

    # End-to-end latency
    total_latency:
      p50_ms:
        maximum: 500
        target: 300
        excellent: 200
      p95_ms:
        maximum: 1000
        target: 600
        excellent: 400
      p99_ms:
        maximum: 2000
        target: 1200
        excellent: 800

  # Throughput requirements
  throughput:
    # Queries per second
    qps:
      minimum: 10
      target: 50
      excellent: 100
    
    # Concurrent requests
    concurrent_requests:
      maximum: 100
      target: 50
      comfortable: 25

  # Resource utilization
  resource_usage:
    # GPU utilization
    gpu_utilization:
      minimum: 0.3   # 30% minimum to justify GPU usage
      maximum: 0.85  # 85% maximum to avoid saturation
      target: 0.70   # 70% target utilization
    
    # GPU memory usage
    gpu_memory:
      maximum_gb: 12
      target_gb: 8
      comfortable_gb: 6
    
    # CPU utilization
    cpu_utilization:
      maximum: 0.80  # 80% maximum
      target: 0.60   # 60% target
      comfortable: 0.40 # 40% comfortable

    # Memory usage
    memory_usage:
      maximum_gb: 32
      target_gb: 16
      comfortable_gb: 8

# ======================
# QUALITY ASSURANCE GATES
# ======================

quality_gates:
  # Pre-production gates
  development:
    required_metrics:
      - ndcg_at_10 >= 0.75
      - mrr >= 0.70
      - rerank_latency_p95_ms <= 200
    required_tests:
      - unit_tests_pass: true
      - integration_tests_pass: true
      - quality_regression_tests_pass: true

  # Staging environment gates
  staging:
    required_metrics:
      - ndcg_at_10 >= 0.80
      - mrr >= 0.75
      - context_precision >= 0.80
      - rerank_latency_p95_ms <= 150
    required_tests:
      - load_tests_pass: true
      - benchmark_tests_pass: true
      - a_b_test_preparation: true
    
  # Production readiness gates
  production:
    required_metrics:
      - ndcg_at_10 >= 0.83
      - mrr >= 0.79
      - context_precision >= 0.85
      - rerank_latency_p95_ms <= 100
      - total_latency_p95_ms <= 800
      - error_rate <= 0.01
    required_tests:
      - security_audit_pass: true
      - scalability_test_pass: true
      - disaster_recovery_test: true
      - monitoring_validation: true

# ======================
# QUALITY MONITORING
# ======================

monitoring_thresholds:
  # Alert thresholds
  alerts:
    critical:
      - ndcg_at_10 < 0.70
      - mrr < 0.65
      - error_rate > 0.05
      - rerank_latency_p95_ms > 500
      - total_latency_p99_ms > 3000
      - gpu_memory_usage > 0.95
    
    warning:
      - ndcg_at_10 < 0.80
      - mrr < 0.75
      - error_rate > 0.02
      - rerank_latency_p95_ms > 200
      - total_latency_p95_ms > 1000
      - gpu_utilization > 0.90

  # Quality degradation detection
  degradation_detection:
    # Sliding window for trend analysis
    window_hours: 24
    
    # Percentage degradation thresholds
    degradation_thresholds:
      ndcg_degradation: 0.05    # 5% drop
      mrr_degradation: 0.08     # 8% drop
      latency_regression: 0.20  # 20% increase
      
    # Minimum number of samples for reliable detection
    minimum_samples: 100

# ======================
# A/B TESTING CRITERIA
# ======================

ab_testing:
  # Statistical significance requirements
  statistical_requirements:
    confidence_level: 0.95      # 95% confidence
    minimum_effect_size: 0.02   # 2% minimum detectable effect
    power: 0.80                 # 80% statistical power
    minimum_sample_size: 1000   # Per variant

  # Success criteria for A/B tests
  success_criteria:
    primary_metrics:
      - metric: ndcg_at_10
        improvement_threshold: 0.05  # 5% improvement
        significance_required: true
      - metric: user_satisfaction
        improvement_threshold: 0.03  # 3% improvement
        significance_required: true
    
    secondary_metrics:
      - metric: total_latency_p95
        regression_threshold: 0.10   # 10% regression allowed
      - metric: cost_per_query
        regression_threshold: 0.15   # 15% cost increase allowed

  # Guardrail metrics (must not regress)
  guardrails:
    - metric: error_rate
      maximum_regression: 0.01     # 1% absolute increase
    - metric: rerank_latency_p95
      maximum_regression_ms: 50    # 50ms increase

# ======================
# EVALUATION DATASETS
# ======================

evaluation_datasets:
  # Primary evaluation sets
  primary:
    # Domain-specific test sets
    - name: "primarch_general_qa"
      size: 1000
      description: "General knowledge Q&A from Primarch knowledge base"
      quality_threshold: 0.85
      
    - name: "primarch_technical_docs"
      size: 500
      description: "Technical documentation retrieval"
      quality_threshold: 0.90
      
    - name: "primarch_multi_hop"
      size: 300
      description: "Multi-hop reasoning queries"
      quality_threshold: 0.75

  # Secondary evaluation sets
  secondary:
    - name: "ms_marco"
      size: 1000
      description: "MS MARCO passage ranking subset"
      quality_threshold: 0.80
      
    - name: "natural_questions"
      size: 800
      description: "Natural Questions subset"
      quality_threshold: 0.78

  # Adversarial test sets
  adversarial:
    - name: "edge_cases"
      size: 200
      description: "Challenging edge cases and failure modes"
      quality_threshold: 0.60
      
    - name: "query_variations"
      size: 300
      description: "Paraphrased and synonym variations"
      quality_threshold: 0.75

# ======================
# QUALITY IMPROVEMENT TARGETS
# ======================

improvement_targets:
  # Short-term targets (1-3 months)
  short_term:
    ndcg_at_10: 0.87      # From 0.83 target
    mrr: 0.82             # From 0.79 target
    rerank_latency_p95: 80 # From 100ms target
    context_precision: 0.88 # From 0.85 target

  # Medium-term targets (3-6 months)
  medium_term:
    ndcg_at_10: 0.90
    mrr: 0.85
    rerank_latency_p95: 60
    context_precision: 0.92
    multilingual_support: true

  # Long-term targets (6-12 months)
  long_term:
    ndcg_at_10: 0.93
    mrr: 0.88
    rerank_latency_p95: 40
    context_precision: 0.95
    real_time_learning: true
    personalization: true

# ======================
# COMPLIANCE & GOVERNANCE
# ======================

compliance:
  # Data quality requirements
  data_quality:
    freshness_hours: 24        # Data must be <24h old
    completeness_threshold: 0.95 # 95% data completeness
    accuracy_threshold: 0.98   # 98% data accuracy
    
  # Model governance
  model_governance:
    version_control: required
    reproducibility: required
    bias_testing: required
    fairness_metrics: required
    
  # Audit requirements
  audit_trail:
    query_logging: required
    result_logging: required
    performance_logging: required
    error_logging: required
    retention_days: 90

# ======================
# EMERGENCY THRESHOLDS
# ======================

emergency_thresholds:
  # Circuit breaker conditions
  circuit_breaker:
    error_rate: 0.10           # 10% error rate
    latency_p99_ms: 5000      # 5 second p99 latency
    consecutive_failures: 5    # 5 consecutive failures
    
  # Auto-fallback conditions
  auto_fallback:
    quality_drop: 0.20         # 20% quality drop
    latency_spike: 2.0         # 2x latency increase
    resource_exhaustion: 0.95  # 95% resource usage

  # Manual intervention triggers
  manual_intervention:
    sustained_degradation_hours: 2
    user_complaint_threshold: 10
    business_impact_severity: "high"
